# 动手学深度学习

$$

$$

## 1. 前言

### 1.1

- `模型`：调整参数后的程序
- `模型族`：通过操作参数而生成的所有不同程序的集合
- `学习算法`：使用数据集来选择参数的元程序
- 训练过程通常包含一下步骤：
  1. 从一个随机初始化参数的模型开始，这个模型基本毫不“智能”。
  2. 获取一些数据样本（例如，音频片段以及对应的是否{是,否}标签）。
  3. 调整参数，使模型在这些样本中表现得更好。
  4. 重复第2步和第3步，直到模型在任务中的表现令你满意。

![../_images/ml-loop.svg](https://zh.d2l.ai/_images/ml-loop.svg)

### 1.2 关键组件

#### 1.2.1 数据

- 每一个数据集有一个个**样本**(example, sample)组成，大多时候，它们遵循独立同分布。样本有时也叫**数据点**或**数据实例**。
- 通常每个样本由一组称为***特征***（features，或***协变量***（covariates））的属性组成。 机器学习模型会根据这些属性进行预测。
- `标签(lable)/目标(target)`：监督学习中预测的特殊的**属性**。
- 数据的`维数（dimensionality）`：每个样本的特征列别数量相同，其特征向量长度固定。
- 数据的准确性、充分性和数据量大小影响着模型的性能。

#### 1.2.2 模型 

- 深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为*深度学习*（deep learning）。

#### 1.2.3 目标函数

- `目标函数`：objective function。模型的优劣程度的度量
- `损失函数`：loss function。用于优化目标函数
  1. 当任务在试图预测数值时，最常见的损失函数是**平方误差**(squared error)，即预测值与实际值之差的平方。
  2.  当试图解决分类问题时，最常见的目标函数是**最小化错误率**，即预测与实际情况不符的样本比例。
- 数据集分成两部分：训练数据集用于拟合模型参数，测试数据集用于评估拟合的模型。

> Loss Function 是定义在单个样本上的，算的是一个样本的误差。
>
> Cost Function 是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。
>
> Object Function（目标函数 ）定义为：Cost Function + 正则化项。

#### 1.2.4 优化算法

- 在深度学习中， 大多优化算法通常基于一种基本方法-梯度下降(gradient descent)。
- 在每个步骤中，梯度下降法都会检查每个参数，在减少损失的方向上优化参数。

### 1.3 各种机器学习问题

#### 1.3.1 监督学习

- `样本`：特征-标签 对
- 监督学习过程

![../_images/supervised-learning.svg](https://zh.d2l.ai/_images/supervised-learning.svg)

##### 1.3.1.1 回归

- `回归`(regression)是最简单的监督学习任务之一，当标签取**任意数值**时，我们称之为**回归**问题。

##### 1.3.1.2 分类

- `分类`（classification）希望模型能够预测样本术语哪个**类别**(category, 正式称为**类**(class))
-  在回归中，我们训练一个回归函数来输出一个数值； 而在分类中，我们训练一个分类器，它的输出即为预测的类别。![img](https://pica.zhimg.com/80/v2-084e4d3cba8917ec5d6984cb5ea669a6_720w.jpg?source=1940ef5c)

- `层次分类`：hierarchical classification。用于寻找层次结构，层次结构假定在许多类之间存在某种关系。 因此，并不是所有的错误都是均等的。  我们宁愿错误地分入一个相关的类别，也不愿错误地分入一个遥远的类别，这通常被称为**层次分类**。

##### 1.3.1.3 标记问题

- `多标签分类`（multi-label classification）：学习预测不互相排斥的类别的问题

##### 1.3.1.4 搜索

- 有时，我们不仅仅希望输出为一个类别或一个实值，而是找到一个集合。比如，网络搜索。

##### 1.3.1.5 推荐系统

- `推荐系统`(recommender system)：目标是向特定用户尽心"个性化"推荐。

##### 1.3.1.6 序列学习

- 在**固定大小**的输入和输出的监督学习中，模型只会将输入作为生成输出的"原料"，并不会"记住"输入的具体内容。
- 当输入是连续的。模型可能需要拥有"**记忆**"功能。
- `序列学习`输入和输出都是可变长度的序列，例如文本和视频。

#### 1.3.2 监督学习

- `无监督学习`：数据中不含有"目标的机器学习问题"
- `聚类`：在没有标签的情况下，进行数据分类
- `主成分分析`：找到少量的参数来准确地捕捉数据的线性相关性
- `因果关系和概率图模型`：描述观察到的许多数据的根本原因
- `生成对抗网络`(generative adversarial networks)：提供一种合成数据的方法

#### 1.3.3 与环境互动

- `离线学习`：offline learning。预先获取大量数据，然后启动模型，不再与环境交互，学习是在算法与环境断开后进行的。![从环境中为监督学习收集数据](https://zh.d2l.ai/_images/data-collection.svg)

​             

#### 1.3.4 强化学习

- 在强化学习问题中，agent在一系列的时间步骤上与环境交互。 在每个特定时间点，agent从环境接收一些**观察**（observation），并且必须选择一个**动作**（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后agent从环境中获得**奖励**（reward）。![../_images/rl-environment.svg](https://zh.d2l.ai/_images/rl-environment.svg)

### 1.4 起源

- `layers（层）`：线性和非线性处理单元的交替
- 使用**链式规则**(**反向传播**，backpropagation)一次性调整网络中的全部参数

### 1.5 深度学习之路

- `dropout`：通过在整个神经网络中应用噪声注入来实现，用随机变量来代替权重，有助于减轻过拟合的危险。
- `注意力机制`在不增加可学习参数的情况下增加系统的记忆和复杂性。
- `多阶段设计`
- `生成对抗网络`使用具有可微参数的任意算法来代替采样器，然后对这些数据进行调整，使得鉴别器（实际上是一个双样本测试）不能区分假数据和真实数据。
- **表示学习**作为机器学习的一类，其研究的重点是如何自动找到合适的数据表示方式。深度学习是通过学习多层次的转换来进行的多层次的表示学习。

## 2. 预备知识